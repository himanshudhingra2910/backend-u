{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class FaceRecognitionSystem:\n",
    "    def __init__(self, similarity_threshold=0.6, tracker_overlap_threshold=0.4):\n",
    "        self.unique_faces = []\n",
    "        self.face_trackers = {}\n",
    "        self.current_face_id = 0\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.tracker_overlap_threshold = tracker_overlap_threshold\n",
    "        \n",
    "        # Load YOLO model for person detection\n",
    "        self.yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "        self.yolo_model.classes = [0]  # Only detect persons (class 0 in COCO dataset)\n",
    "    \n",
    "    def detect_persons(self, frame):\n",
    "        \"\"\"Detect persons in the frame using YOLO.\"\"\"\n",
    "        results = self.yolo_model(frame)\n",
    "        detections = results.xyxy[0].cpu().numpy()\n",
    "        \n",
    "        person_boxes = []\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2, conf, cls = detection\n",
    "            if cls == 0:  # Class 0 is 'person'\n",
    "                person_boxes.append((int(x1), int(y1), int(x2), int(y2)))\n",
    "        \n",
    "        return person_boxes\n",
    "    \n",
    "    def is_unique(self, embedding, threshold=None):\n",
    "        \"\"\"Check if a face embedding is unique compared to stored embeddings.\"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = self.similarity_threshold\n",
    "            \n",
    "        embeddings_list = [f['embedding'] for f in self.unique_faces]\n",
    "        \n",
    "        if len(embeddings_list) == 0:\n",
    "            return True  # If no embeddings exist, it's the first unique face\n",
    "        \n",
    "        distances = euclidean_distances([embedding], embeddings_list)\n",
    "        \n",
    "        return np.min(distances) > threshold\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a video frame to detect faces and extract embeddings.\"\"\"\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "        face_locations = face_recognition.face_locations(rgb_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "        \n",
    "        return face_encodings, face_locations\n",
    "    \n",
    "    def create_face_tracker(self, frame, location):\n",
    "        \"\"\"Initialize a new face tracker for a given face location.\"\"\"\n",
    "        tracker = cv2.TrackerCSRT_create()\n",
    "        top, right, bottom, left = location\n",
    "        bbox = (left, top, right - left, bottom - top)\n",
    "        tracker.init(frame, bbox)\n",
    "        return tracker\n",
    "\n",
    "    def track_faces(self, frame, draw=True):\n",
    "        \"\"\"Track faces over time and optionally draw bounding boxes.\"\"\"\n",
    "        updated_trackers = {}\n",
    "        for face_id, tracker in self.face_trackers.items():\n",
    "            success, bbox = tracker.update(frame)\n",
    "            if success:\n",
    "                updated_trackers[face_id] = tracker\n",
    "                \n",
    "                if draw:\n",
    "                    (left, top, width, height) = [int(v) for v in bbox]\n",
    "                    cv2.rectangle(frame, (left, top), (left + width, top + height), (255, 0, 0), 2)\n",
    "                    cv2.putText(frame, f\"ID {face_id}\", (left, top - 10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        self.face_trackers = updated_trackers\n",
    "        return frame\n",
    "    \n",
    "    def bbox_overlap(self, bbox1, bbox2):\n",
    "        \"\"\"Calculate overlap between two bounding boxes (IOU).\"\"\"\n",
    "        left1, top1, width1, height1 = bbox1\n",
    "        left2, top2, width2, height2 = bbox2\n",
    "        \n",
    "        # Calculate overlap area\n",
    "        x_overlap = max(0, min(left1 + width1, left2 + width2) - max(left1, left2))\n",
    "        y_overlap = max(0, min(top1 + height1, top2 + height2) - max(top1, top2))\n",
    "        overlap_area = x_overlap * y_overlap\n",
    "        \n",
    "        bbox1_area = width1 * height1\n",
    "        bbox2_area = width2 * height2\n",
    "        \n",
    "        # Calculate IOU\n",
    "        iou = overlap_area / float(bbox1_area + bbox2_area - overlap_area)\n",
    "        return iou\n",
    "    \n",
    "    def process_video(self, video_path, skip_frames=10, display=True, verbose=True):\n",
    "        \"\"\"Process a video and count unique faces.\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        frame_count = 0\n",
    "        unique_faces_count = 0\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # Exit loop if video ends\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # Detect persons using YOLO\n",
    "            person_boxes = self.detect_persons(frame)\n",
    "            \n",
    "            for person_box in person_boxes:\n",
    "                x1, y1, x2, y2 = person_box\n",
    "                person_frame = frame[y1:y2, x1:x2]  # Crop the person region\n",
    "                \n",
    "                # Update trackers and draw bounding boxes\n",
    "                frame = self.track_faces(frame, draw=display)\n",
    "                \n",
    "                if frame_count % skip_frames == 0:\n",
    "                    face_encodings, face_locations = self.process_frame(person_frame)\n",
    "                    \n",
    "                    for encoding, location in zip(face_encodings, face_locations):\n",
    "                        # Adjust face location to the original frame coordinates\n",
    "                        adjusted_location = (\n",
    "                            location[0] + y1,\n",
    "                            location[1] + x1,\n",
    "                            location[2] + y1,\n",
    "                            location[3] + x1\n",
    "                        )\n",
    "                        \n",
    "                        face_bbox = (\n",
    "                            adjusted_location[3],\n",
    "                            adjusted_location[0],\n",
    "                            adjusted_location[1] - adjusted_location[3],\n",
    "                            adjusted_location[2] - adjusted_location[0]\n",
    "                        )\n",
    "                        \n",
    "                        is_new_face = True\n",
    "                        for face_id, tracker in self.face_trackers.items():\n",
    "                            success, tracked_bbox = tracker.update(frame)\n",
    "                            if success and self.bbox_overlap(face_bbox, tracked_bbox) > self.tracker_overlap_threshold:\n",
    "                                is_new_face = False\n",
    "                                break\n",
    "                        \n",
    "                        if is_new_face and self.is_unique(encoding):\n",
    "                            face_tracker = self.create_face_tracker(frame, adjusted_location)\n",
    "                            self.face_trackers[self.current_face_id] = face_tracker\n",
    "                            self.unique_faces.append({'id': self.current_face_id, 'embedding': encoding})\n",
    "                            self.current_face_id += 1\n",
    "                            unique_faces_count += 1\n",
    "            \n",
    "            if verbose and frame_count % 50 == 0:\n",
    "                print(f\"Processed {frame_count} frames, Unique faces so far: {unique_faces_count}\")\n",
    "            \n",
    "            if display:\n",
    "                cv2.imshow('Frame', frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        return unique_faces_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    face_system = FaceRecognitionSystem(\n",
    "        similarity_threshold=0.6,\n",
    "        tracker_overlap_threshold=0.4\n",
    "    )\n",
    "    \n",
    "    video_file = 'exampleVideo.mp4'\n",
    "    total_unique_faces = face_system.process_video(\n",
    "        video_file,\n",
    "        skip_frames=10,\n",
    "        display=True,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Total number of unique faces detected: {total_unique_faces}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Himanshu.Dhingra/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['gitpython>=3.1.30', 'pillow>=10.3.0'] not found, attempting AutoUpdate...\n",
      "Requirement already satisfied: gitpython>=3.1.30 in c:\\users\\himanshu.dhingra\\appdata\\local\\anaconda3\\lib\\site-packages (3.1.43)\n",
      "Requirement already satisfied: pillow>=10.3.0 in c:\\users\\himanshu.dhingra\\appdata\\local\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\himanshu.dhingra\\appdata\\local\\anaconda3\\lib\\site-packages (from gitpython>=3.1.30) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\himanshu.dhingra\\appdata\\local\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30) (4.0.0)\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  8.1s, installed 2 packages: ['gitpython>=3.1.30', 'pillow>=10.3.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2025-2-25 Python-3.8.20 torch-2.2.2+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50 frames, Unique faces so far: 1\n",
      "Processed 100 frames, Unique faces so far: 1\n",
      "Processed 150 frames, Unique faces so far: 1\n",
      "Processed 200 frames, Unique faces so far: 1\n",
      "Processed 250 frames, Unique faces so far: 1\n",
      "Processed 300 frames, Unique faces so far: 1\n",
      "Processed 350 frames, Unique faces so far: 1\n",
      "Processed 400 frames, Unique faces so far: 1\n",
      "Processed 450 frames, Unique faces so far: 1\n",
      "Processed 500 frames, Unique faces so far: 1\n",
      "Processed 550 frames, Unique faces so far: 1\n",
      "Processed 600 frames, Unique faces so far: 1\n",
      "Processed 650 frames, Unique faces so far: 1\n",
      "Processed 700 frames, Unique faces so far: 1\n",
      "Processed 750 frames, Unique faces so far: 1\n",
      "Processed 800 frames, Unique faces so far: 1\n",
      "Processed 850 frames, Unique faces so far: 1\n",
      "Processed 900 frames, Unique faces so far: 1\n",
      "Processed 950 frames, Unique faces so far: 1\n",
      "Processed 1000 frames, Unique faces so far: 2\n",
      "Total number of unique faces detected: 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faceRecognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
